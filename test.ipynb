{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/German_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Good/Bad'] = data['Good/Bad'].astype(int)\n",
    "data['sex'] = LabelEncoder().fit_transform(data['sex']) \n",
    "# 区分数值型和分类型变量\n",
    "numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = data.select_dtypes(include=['object']).columns.tolist()\n",
    "# print(categorical_features)\n",
    "# 去掉good/bad 和sex列\n",
    "numerical_features.remove('Good/Bad')\n",
    "numerical_features.remove('sex')\n",
    "#对分类变量进行独热编码\n",
    "data_encoded = pd.get_dummies(data[categorical_features], drop_first=True)\n",
    "data_encoded = data_encoded.astype(float)\n",
    "data_preprocessed = pd.concat([data[numerical_features], data_encoded], axis=1)\n",
    "# 分离特征和标签\n",
    "X = data_preprocessed\n",
    "y_label = data['Good/Bad']\n",
    "y_sensitive = data['sex']\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_label_train, y_label_test, y_sensitive_train, y_sensitive_test = train_test_split(\n",
    "    X_scaled, y_label, y_sensitive, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.24755338,  1.52682862, -0.86919627,  1.04808574, -0.39839302,\n",
       "       -0.7039176 , -0.42854322,  1.64734776, -0.25943726, -0.80699422,\n",
       "       -0.22710999, -1.06304467,  3.21749479, -0.64266066, -0.33905067,\n",
       "        9.06917857, -0.47039493, -0.62249498, -0.11026357, -0.15005969,\n",
       "       -0.22953657, -0.09534626, -0.32793109,  2.94941166, -0.25943726,\n",
       "       -0.22466238, -0.47198116, -0.45604896, -0.71668428, -0.4592484 ,\n",
       "       -0.58081787,  0.6707663 , -0.20687555, -0.23432935, -0.54997926,\n",
       "       -0.70551479,  2.3424374 , -0.22219304,  0.47831191, -1.57506706,\n",
       "        2.87228132, -0.50031279, -1.30384048,  2.39791576,  1.21610369,\n",
       "       -0.19611614])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:11:17.691857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# # 标签注释器\n",
    "# input_dim = X_train.shape[1]\n",
    "# label_input = Input(shape=(input_dim,), name='label_input')\n",
    "# h = Dense(64, activation='relu')(label_input)\n",
    "# h = Dense(32, activation='relu')(h)\n",
    "# label_output = Dense(1, activation='sigmoid', name='label_output')(h)  # 这里添加(h)\n",
    "\n",
    "# label_model = Model(inputs=label_input, outputs=label_output)\n",
    "# label_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# label_model.fit(X_train, y_label_train, epochs=100, batch_size=16, validation_split=0.25)\n",
    "# label_model.evaluate(X_test, y_label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 敏感属性注释器\n",
    "# sensitive_input = Input(shape=(input_dim,), name='sensitive_input')\n",
    "# h = Dense(64, activation='relu')(sensitive_input)\n",
    "# h = Dense(32, activation='relu')(h)\n",
    "# sensitive_output = Dense(1, activation='sigmoid', name='sensitive_output')(h)  # 这里添加(h)\n",
    "\n",
    "# sensitive_model = Model(inputs=sensitive_input, outputs=sensitive_output)\n",
    "# sensitive_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# sensitive_model.fit(X_train, y_sensitive_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "# sensitive_model.evaluate(X_test, y_sensitive_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "# from tensorflow.keras.models import Model\n",
    "# # # 生成器\n",
    "# # generator_input = Input(shape=(input_dim,), name='generator_input')\n",
    "# # h = Dense(128)(generator_input)\n",
    "# # h = LeakyReLU(negative_slope=0.01)(h)  # Use LeakyReLU for better gradient flow\n",
    "# # h = Dense(256)(h)\n",
    "# # h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# # h = Dense(512)(h)\n",
    "# # h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# # generator_output = Dense(input_dim, activation='linear', name='generator_output')(h)\n",
    "\n",
    "\n",
    "# # generator_model = Model(inputs=generator_input, outputs=generator_output)\n",
    "# # generator_model.compile(optimizer='adam', loss='mean_squared_error')  # 编译生成器模型\n",
    "\n",
    "# # # 在训练循环中，训练生成器\n",
    "# # noise = np.random.normal(0, 1, size=(len(X_train), input_dim))\n",
    "# # generator_loss = generator_model.train_on_batch(noise, X_train)\n",
    "# # print(\"Generator Loss:\", generator_loss)\n",
    "\n",
    "# # 生成器\n",
    "# generator_input = Input(shape=(input_dim,), name='generator_input')\n",
    "# h = Dense(128)(generator_input)\n",
    "# h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# h = Dense(256)(h)\n",
    "# h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# h = Dense(512)(h)\n",
    "# h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# generator_output = Dense(input_dim, activation='linear', name='generator_output')(h)\n",
    "\n",
    "# generator_model = Model(inputs=generator_input, outputs=generator_output)\n",
    "# generator_model.compile(optimizer='adam', loss='mean_squared_error')  # 编译生成器模型\n",
    "# # 在训练循环中，训练生成器\n",
    "# noise = np.random.normal(0, 1, size=(len(X_train), input_dim))\n",
    "# generator_loss = generator_model.train_on_batch(noise, X_train)\n",
    "# print(\"Generator Loss:\", generator_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 分类器\n",
    "# classifier_input = Input(shape=(input_dim,), name='classifier_input')\n",
    "# h = Dense(128)(classifier_input)\n",
    "# h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# h = Dense(256)(h)\n",
    "# h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# h = Dense(512)(h)\n",
    "# h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# h = Dense(256)(h)\n",
    "# h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# h = Dense(128)(h)\n",
    "# h = LeakyReLU(negative_slope=0.01)(h)\n",
    "# classifier_output = Dense(1, activation='sigmoid', name='classifier_output')(h)\n",
    "\n",
    "# classifier_model = Model(inputs=classifier_input, outputs=classifier_output)\n",
    "# classifier_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# classifier_loss = classifier_model.train_on_batch(X_train, y_label_train)\n",
    "# print(classifier_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # 定义交替训练的轮数和批次大小\n",
    "# epochs = 100\n",
    "# batch_size = 16\n",
    "\n",
    "# # 交替训练生成器和分类器\n",
    "# for epoch in range(epochs):\n",
    "#     print(\"Epoch:\", epoch + 1)\n",
    "    \n",
    "#     # 训练生成器\n",
    "#     # 1. 使用标签注释器和敏感属性注释器生成标签和敏感属性的预测\n",
    "#     label_predictions = label_model.predict(X_train)\n",
    "#     sensitive_predictions = sensitive_model.predict(X_train)\n",
    "    \n",
    "#     # 2. 生成随机噪声\n",
    "#     noise = np.random.normal(0, 1, size=(len(X_train), input_dim))\n",
    "    \n",
    "#     # 3. 使用生成器生成新的数据样本\n",
    "#     generated_data = generator_model.predict(noise) \n",
    "    \n",
    "#     # 4. 构建生成器训练数据，包括标签注释器和敏感属性注释器的预测结果\n",
    "#     generator_input = np.concatenate([label_predictions, sensitive_predictions, noise], axis=1)\n",
    "    \n",
    "#     # 5. 训练生成器\n",
    "#     generator_loss = generator_model.train_on_batch(noise, X_train)\n",
    "#     # print(\"Generator Loss:\", generator_loss)\n",
    "    \n",
    "#     # 训练分类器\n",
    "#     # 1. 使用生成器生成的数据样本和真实数据样本构建训练数据\n",
    "#     combined_data = np.concatenate([generated_data, X_train], axis=0)\n",
    "#     combined_labels = np.concatenate([np.zeros(len(generated_data)), np.ones(len(X_train))])\n",
    "    \n",
    "#     # 2. 训练分类器\n",
    "#     classifier_loss = classifier_model.train_on_batch(combined_data, combined_labels)\n",
    "#     print(\"Generator Loss:\", generator_loss)\n",
    "#     print(\"Classifier Loss:\", classifier_loss)\n",
    "#     # 每一百次打印一次结果\n",
    "#     # if (epoch + 1) % 100 == 0:\n",
    "#         # print(\"Generator Loss:\", generator_loss)\n",
    "#         # print(\"Classifier Loss:\", classifier_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用生成器生成新的数据样本\n",
    "# generated_samples = generator_model.predict(noise)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行结果：注释器训练次数30，生成器/分类器训练次数1000，效果accuracy = 0\n",
    "修改：尝试理解其中的结构：重新定义分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用分类器对生成的数据进行分类\n",
    "# generated_labels = classifier_model.predict(generated_samples)\n",
    "# print(generated_labels)\n",
    "# # 计算分类器的性能指标\n",
    "# # accuracy = classifier_model.evaluate(generated_samples, generated_labels)[1]\n",
    "# # print(\"Classifier Accuracy on Generated Data:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# # 更新生成器\n",
    "# def build_generator(input_dim):\n",
    "#     generator_input = Input(shape=(input_dim,), name='generator_input')\n",
    "#     h = Dense(256)(generator_input)\n",
    "#     h = LeakyReLU(negative_slope=0.01)(h)\n",
    "#     h = Dense(512)(h)\n",
    "#     h = LeakyReLU(negative_slope=0.01)(h)\n",
    "#     h = Dense(1024)(h)\n",
    "#     h = LeakyReLU(negative_slope=0.01)(h)\n",
    "#     generator_output = Dense(input_dim, activation='linear', name='generator_output')(h)\n",
    "#     generator_model = Model(inputs=generator_input, outputs=generator_output)\n",
    "#     return generator_model\n",
    "\n",
    "# # 更新分类器\n",
    "# def build_classifier(input_dim):\n",
    "#     classifier_input = Input(shape=(input_dim,), name='classifier_input')\n",
    "#     h = Dense(128)(classifier_input)\n",
    "#     h = LeakyReLU(negative_slope=0.01)(h)\n",
    "#     h = Dense(256)(h)\n",
    "#     h = LeakyReLU(negative_slope=0.01)(h)\n",
    "#     h = Dense(512)(h)\n",
    "#     h = LeakyReLU(negative_slope=0.01)(h)\n",
    "#     h = Dense(256)(h)\n",
    "#     h = LeakyReLU(negative_slope=0.01)(h)\n",
    "#     h = Dense(128)(h)\n",
    "#     h = LeakyReLU(negative_slope=0.01)(h)\n",
    "#     classifier_output = Dense(1, activation='sigmoid', name='classifier_output')(h)\n",
    "#     classifier_model = Model(inputs=classifier_input, outputs=classifier_output)\n",
    "#     return classifier_model\n",
    "\n",
    "# input_dim = X_train.shape[1]\n",
    "\n",
    "# # 构建生成器和分类器\n",
    "# generator_model = build_generator(input_dim)\n",
    "# classifier_model = build_classifier(input_dim)\n",
    "\n",
    "# # 编译分类器\n",
    "# classifier_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # 构建联合模型：生成器 + 分类器\n",
    "# classifier_model.trainable = False\n",
    "# gan_input = Input(shape=(input_dim,))\n",
    "# generated_data = generator_model(gan_input)\n",
    "# gan_output = classifier_model(generated_data)\n",
    "# gan_model = Model(gan_input, gan_output)\n",
    "# gan_model.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "\n",
    "# # 定义交替训练的轮数和批次大小\n",
    "# epochs = 1000\n",
    "# batch_size = 32\n",
    "\n",
    "# # 存储损失\n",
    "# generator_losses = []\n",
    "# classifier_losses = []\n",
    "\n",
    "# # 交替训练生成器和分类器\n",
    "# for epoch in range(epochs):\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(\"Epoch:\", epoch + 1)\n",
    "    \n",
    "#     # 训练分类器\n",
    "#     # 1. 生成真实数据和生成数据的标签\n",
    "#     label_predictions = label_model.predict(X_train, verbose=0)\n",
    "#     sensitive_predictions = sensitive_model.predict(X_train, verbose=0)\n",
    "    \n",
    "#     # 2. 生成随机噪声\n",
    "#     noise = np.random.normal(0, 1, size=(len(X_train), input_dim))\n",
    "    \n",
    "#     # 3. 使用生成器生成新的数据样本\n",
    "#     generated_data = generator_model.predict(noise, verbose=0)\n",
    "    \n",
    "#     # 4. 构建分类器训练数据\n",
    "#     combined_data = np.concatenate([generated_data, X_train], axis=0)\n",
    "#     combined_labels = np.concatenate([np.zeros(len(generated_data)), np.ones(len(X_train))])\n",
    "    \n",
    "#     # 5. 训练分类器\n",
    "#     classifier_loss = classifier_model.train_on_batch(combined_data, combined_labels)\n",
    "    \n",
    "#     # 训练生成器\n",
    "#     # 1. 生成随机噪声\n",
    "#     noise = np.random.normal(0, 1, size=(batch_size, input_dim))\n",
    "    \n",
    "#     # 2. 训练生成器（使生成的数据被分类器认为是真实的）\n",
    "#     generator_loss = gan_model.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "    \n",
    "#     # 存储损失\n",
    "#     generator_losses.append(generator_loss)\n",
    "#     classifier_losses.append(classifier_loss[0])\n",
    "    \n",
    "#     # 每一百次打印一次结果\n",
    "#     if (epoch + 1) % 100 == 0:\n",
    "#         print(\"Generator Loss:\", generator_loss)\n",
    "#         print(\"Classifier Loss:\", classifier_loss)\n",
    "\n",
    "# # 可视化训练过程中的损失\n",
    "# plt.plot(generator_losses, label='Generator Loss')\n",
    "# plt.plot(classifier_losses, label='Classifier Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # 生成样本数据\n",
    "# generated_samples = generator_model.predict(np.random.normal(0, 1, size=(10, input_dim)))\n",
    "\n",
    "# # 使用分类器对生成的数据进行分类\n",
    "# generated_labels = classifier_model.predict(generated_samples)\n",
    "# print(generated_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成样本数据\n",
    "# generated_samples = generator_model.predict(np.random.normal(0, 1, size=(10, input_dim)))\n",
    "# print(generated_samples)\n",
    "# print('here the original data')\n",
    "# print(X_train)\n",
    "\n",
    "# # # 使用分类器对生成的数据进行分类\n",
    "# # generated_labels = classifier_model.predict(generated_samples)\n",
    "# # print(generated_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码效果不好，分析原因：分类器只是进行了分类，并没有告诉生成器生成的数据是好是坏，因此不能够得到收敛的结果\n",
    "但是如果不这样，直接还是使用判别器，我应当如何训练分类器？难道是GAN作为生成的一部分，然后每次循环当中再加上分类器的训练？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.705, Precision: 0.7294117647058823, Recall: 0.9051094890510949, F1 Score: 0.8078175895765473\n"
     ]
    }
   ],
   "source": [
    "#分类器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 训练分类器\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_label_train)\n",
    "\n",
    "# 预测\n",
    "y_label_pred = classifier.predict(X_test)\n",
    "\n",
    "# 评估分类器性能\n",
    "accuracy = accuracy_score(y_label_test, y_label_pred)\n",
    "precision = precision_score(y_label_test, y_label_pred)\n",
    "recall = recall_score(y_label_test, y_label_pred)\n",
    "f1 = f1_score(y_label_test, y_label_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5000], Loss D: 1.3731852769851685, Loss G: 0.683114767074585\n",
      "Epoch [100/5000], Loss D: 0.021458474919199944, Loss G: 3.971863269805908\n",
      "Epoch [200/5000], Loss D: 0.009374379180371761, Loss G: 8.97299575805664\n",
      "Epoch [300/5000], Loss D: 0.00019671082554850727, Loss G: 11.472001075744629\n",
      "Epoch [400/5000], Loss D: 4.0322055383512634e-07, Loss G: 15.104263305664062\n",
      "Epoch [500/5000], Loss D: 2.9477760108420625e-05, Loss G: 10.691465377807617\n",
      "Epoch [600/5000], Loss D: 1.374008661514381e-05, Loss G: 11.4887113571167\n",
      "Epoch [700/5000], Loss D: 0.03237169235944748, Loss G: 18.481685638427734\n",
      "Epoch [800/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.462650299072266\n",
      "Epoch [900/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.46120071411133\n",
      "Epoch [1000/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.46110534667969\n",
      "Epoch [1100/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.46107482910156\n",
      "Epoch [1200/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.461063385009766\n",
      "Epoch [1300/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.461002349853516\n",
      "Epoch [1400/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.46091842651367\n",
      "Epoch [1500/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.460670471191406\n",
      "Epoch [1600/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.46056365966797\n",
      "Epoch [1700/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.460445404052734\n",
      "Epoch [1800/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.46000671386719\n",
      "Epoch [1900/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.459373474121094\n",
      "Epoch [2000/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.458282470703125\n",
      "Epoch [2100/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.454307556152344\n",
      "Epoch [2200/5000], Loss D: 1.4919816770930794e-10, Loss G: 37.34559631347656\n",
      "Epoch [2300/5000], Loss D: 1.4919955548808872e-10, Loss G: 34.170021057128906\n",
      "Epoch [2400/5000], Loss D: 1.4919955548808872e-10, Loss G: 34.16917037963867\n",
      "Epoch [2500/5000], Loss D: 1.4919956936587653e-10, Loss G: 34.16688537597656\n",
      "Epoch [2600/5000], Loss D: 1.4919962487702776e-10, Loss G: 34.12321853637695\n",
      "Epoch [2700/5000], Loss D: 1.5307347056570109e-10, Loss G: 26.27638053894043\n",
      "Epoch [2800/5000], Loss D: 3.3023209198290715e-06, Loss G: 27.48366355895996\n",
      "Epoch [2900/5000], Loss D: 0.002476755529642105, Loss G: 19.724409103393555\n",
      "Epoch [3000/5000], Loss D: 2.68556621385585e-09, Loss G: 42.80088424682617\n",
      "Epoch [3100/5000], Loss D: 2.68556621385585e-09, Loss G: 42.670291900634766\n",
      "Epoch [3200/5000], Loss D: 2.68556621385585e-09, Loss G: 42.5105094909668\n",
      "Epoch [3300/5000], Loss D: 2.68556621385585e-09, Loss G: 42.27693176269531\n",
      "Epoch [3400/5000], Loss D: 2.68556621385585e-09, Loss G: 41.927772521972656\n",
      "Epoch [3500/5000], Loss D: 2.68556621385585e-09, Loss G: 41.38491439819336\n",
      "Epoch [3600/5000], Loss D: 2.68556621385585e-09, Loss G: 40.13809585571289\n",
      "Epoch [3700/5000], Loss D: 2.685566435900455e-09, Loss G: 36.69889450073242\n",
      "Epoch [3800/5000], Loss D: 2.685566435900455e-09, Loss G: 36.453125\n",
      "Epoch [3900/5000], Loss D: 2.685566435900455e-09, Loss G: 36.429969787597656\n",
      "Epoch [4000/5000], Loss D: 2.6856297186128586e-09, Loss G: 30.38799476623535\n",
      "Epoch [4100/5000], Loss D: 2.969990919865495e-09, Loss G: 21.981042861938477\n",
      "Epoch [4200/5000], Loss D: 5.878418107840844e-08, Loss G: 67.46222686767578\n",
      "Epoch [4300/5000], Loss D: 3.506159274024867e-08, Loss G: 67.62357330322266\n",
      "Epoch [4400/5000], Loss D: 3.4912396529307443e-08, Loss G: 67.6234359741211\n",
      "Epoch [4500/5000], Loss D: 3.4912396529307443e-08, Loss G: 67.62333679199219\n",
      "Epoch [4600/5000], Loss D: 3.4912396529307443e-08, Loss G: 67.62326049804688\n",
      "Epoch [4700/5000], Loss D: 3.4912396529307443e-08, Loss G: 67.62310028076172\n",
      "Epoch [4800/5000], Loss D: 3.476319676565254e-08, Loss G: 67.62299346923828\n",
      "Epoch [4900/5000], Loss D: 3.461400055471131e-08, Loss G: 67.62281036376953\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, X_train.shape[1]),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "netG = Generator()\n",
    "netD = Discriminator()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002)\n",
    "\n",
    "# 训练GAN\n",
    "for epoch in range(5000):\n",
    "    # 更新判别器\n",
    "    netD.zero_grad()\n",
    "    real_data = torch.Tensor(X_train)\n",
    "    batch_size = real_data.size(0)\n",
    "    labels = torch.ones((batch_size, 1))  # 创建一个形状为(batch_size, 1)的张量\n",
    "    output = netD(real_data)\n",
    "    lossD_real = criterion(output, labels)\n",
    "    lossD_real.backward()\n",
    "\n",
    "    # noise = torch.randn(batch_size, 100)\n",
    "    # 例如，将均匀分布和正态分布进行混合\n",
    "    uniform_noise = torch.rand(batch_size, 50)  # 生成均匀分布的随机数\n",
    "    normal_noise = torch.randn(batch_size, 50)  # 生成正态分布的随机数\n",
    "    noise = torch.cat((uniform_noise, normal_noise), dim=1)  # 将两种噪声分布进行拼接\n",
    "\n",
    "    fake_data = netG(noise)\n",
    "    labels = torch.zeros(batch_size,1)\n",
    "    output = netD(fake_data.detach())\n",
    "    lossD_fake = criterion(output, labels)\n",
    "    lossD_fake.backward()\n",
    "    \n",
    "\n",
    "    optimizerD.step()\n",
    "\n",
    "    # 更新生成器\n",
    "    netG.zero_grad()\n",
    "    labels = torch.ones(batch_size,1)\n",
    "    output = netD(fake_data)\n",
    "    lossG = criterion(output, labels)\n",
    "    lossG.backward()\n",
    "    optimizerG.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/5000], Loss D: {lossD_real + lossD_fake}, Loss G: {lossG}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -1.         -1.\n",
      "  -1.         -1.          1.         -1.          1.         -1.\n",
      "   1.         -1.         -1.         -1.         -1.          1.\n",
      "   1.          1.          1.          1.         -1.          1.\n",
      "  -1.         -1.         -1.         -1.         -1.          1.\n",
      "  -1.          1.         -1.          1.         -1.          1.\n",
      "   1.          1.         -1.          1.        ]\n",
      " [ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -1.         -1.\n",
      "  -1.         -1.          1.         -1.          1.         -1.\n",
      "   1.         -1.         -1.         -1.         -1.          1.\n",
      "   0.9999997   1.          1.          1.         -1.          0.9999999\n",
      "  -1.         -1.         -1.         -1.         -1.          1.\n",
      "  -1.          1.         -1.          0.99999994 -1.          1.\n",
      "   1.          1.         -1.          1.        ]\n",
      " [ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -1.         -1.\n",
      "  -1.         -1.          1.         -1.          1.         -1.\n",
      "   1.         -1.         -1.         -1.         -1.          1.\n",
      "   1.          1.          1.          1.         -1.          1.\n",
      "  -1.         -1.         -1.         -1.         -1.          1.\n",
      "  -1.          1.         -1.          1.         -1.          1.\n",
      "   1.          1.         -1.          1.        ]\n",
      " [ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -1.         -1.\n",
      "  -1.         -1.          1.         -1.          1.         -1.\n",
      "   1.         -1.         -1.         -1.         -1.          1.\n",
      "   1.          1.          1.          1.         -1.          1.\n",
      "  -1.         -1.         -1.         -1.         -1.          1.\n",
      "  -1.          1.         -1.          1.         -1.          1.\n",
      "   1.          1.         -1.          1.        ]\n",
      " [ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -1.         -1.\n",
      "  -1.         -1.          1.         -1.          1.         -1.\n",
      "   1.         -1.         -1.         -1.         -1.          1.\n",
      "   1.          1.          1.          1.         -1.          1.\n",
      "  -1.         -1.         -1.         -1.         -1.          1.\n",
      "  -1.          1.         -1.          1.         -1.          1.\n",
      "   1.          1.         -1.          1.        ]\n",
      " [ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -0.9999999  -1.\n",
      "  -1.         -1.          1.         -0.99999994  1.         -1.\n",
      "   1.         -1.         -1.         -0.9999999  -1.          1.\n",
      "   0.9999984   1.          1.          1.         -1.          0.9999995\n",
      "  -1.         -1.         -1.         -1.         -0.9999999   1.\n",
      "  -1.          1.         -1.          0.9999997  -1.          1.\n",
      "   1.          1.         -1.          1.        ]\n",
      " [ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -1.         -1.\n",
      "  -1.         -1.          1.         -1.          1.         -1.\n",
      "   1.         -1.         -1.         -1.         -1.          1.\n",
      "   1.          1.          1.          1.         -1.          1.\n",
      "  -1.         -1.         -1.         -1.         -1.          1.\n",
      "  -1.          1.         -1.          1.         -1.          1.\n",
      "   1.          1.         -1.          1.        ]\n",
      " [ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -1.         -1.\n",
      "  -1.         -1.          1.         -1.          1.         -1.\n",
      "   1.         -1.         -1.         -1.         -1.          1.\n",
      "   0.99999994  1.          1.          1.         -1.          1.\n",
      "  -1.         -1.         -1.         -1.         -1.          1.\n",
      "  -1.          1.         -1.          1.         -1.          1.\n",
      "   1.          1.         -1.          1.        ]\n",
      " [ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -1.         -1.\n",
      "  -1.         -1.          1.         -1.          1.         -1.\n",
      "   1.         -1.         -1.         -1.         -1.          1.\n",
      "   1.          1.          1.          1.         -1.          1.\n",
      "  -1.         -1.         -1.         -1.         -1.          1.\n",
      "  -1.          1.         -1.          1.         -1.          1.\n",
      "   1.          1.         -1.          1.        ]\n",
      " [ 1.         -1.         -1.          1.          1.          1.\n",
      "   1.          1.          1.          1.         -1.         -1.\n",
      "  -1.         -1.          1.         -1.          1.         -1.\n",
      "   1.         -1.         -1.         -1.         -1.          1.\n",
      "   1.          1.          1.          1.         -1.          1.\n",
      "  -1.         -1.         -1.         -1.         -1.          1.\n",
      "  -1.          1.         -1.          1.         -1.          1.\n",
      "   1.          1.         -1.          1.        ]]\n",
      "[[-0.24215924  0.21213598 -0.86919627 ... -0.41702883  1.21610369\n",
      "  -0.19611614]\n",
      " [ 2.24755338  1.52682862 -0.86919627 ...  2.39791576  1.21610369\n",
      "  -0.19611614]\n",
      " [-0.24215924 -0.75917737  0.91932499 ... -0.41702883 -0.8222983\n",
      "  -0.19611614]\n",
      " ...\n",
      " [-0.24215924 -0.39134692  0.91932499 ... -0.41702883  1.21610369\n",
      "  -0.19611614]\n",
      " [-1.23804428 -0.92608309 -0.86919627 ... -0.41702883 -0.8222983\n",
      "  -0.19611614]\n",
      " [-0.98907302 -0.47993807  0.91932499 ... -0.41702883  1.21610369\n",
      "  -0.19611614]]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 生成数据\n",
    "import numpy as np\n",
    "noise = torch.randn(10, 100)\n",
    "generated_data = netG(noise).detach().numpy()\n",
    "print(generated_data)\n",
    "print(X_train)\n",
    "# 注释生成的数据\n",
    "def label_annotator(data, classifier):\n",
    "    return classifier.predict(data)\n",
    "\n",
    "labels = label_annotator(generated_data, classifier)\n",
    "print(labels)\n",
    "# 敏感类型注释器\n",
    "def sensitive_type_annotator(data):\n",
    "    # 假设敏感信息在数据的特定列\n",
    "    # 在这个例子中，假设最后一列是敏感信息（性别）\n",
    "    sensitive_info = np.zeros(data.shape[0])  # 假设所有生成数据的敏感类型都为0\n",
    "    return sensitive_info\n",
    "\n",
    "sensitive_info = sensitive_type_annotator(generated_data)\n",
    "# print(sensitive_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0, label distribution: 1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_fairness(labels, sensitive_info):\n",
    "    sensitive_groups = np.unique(sensitive_info)\n",
    "    for group in sensitive_groups:\n",
    "        group_labels = labels[sensitive_info == group]\n",
    "        print(f'Group {group}, label distribution: {np.mean(group_labels)}')\n",
    "\n",
    "evaluate_fairness(labels, sensitive_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'c_compute_loss',\n",
       " 'c_train_step',\n",
       " 'c_update',\n",
       " 'compute_loss',\n",
       " 'generated_population_stats',\n",
       " 'get_mu_vars',\n",
       " 'init_classifier',\n",
       " 'init_gen_opt',\n",
       " 'load',\n",
       " 'predict',\n",
       " 'reparation_batch',\n",
       " 'sample_decoder',\n",
       " 'save',\n",
       " 'train_classifier',\n",
       " 'train_generator',\n",
       " 'train_step',\n",
       " 'update',\n",
       " 'validate',\n",
       " 'visualize_outputs']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import train_fn\n",
    "dir(train_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
